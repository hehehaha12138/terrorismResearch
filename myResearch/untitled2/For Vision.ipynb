{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "def Show2dCorpora(corpus):\n",
    "    nodes = list(corpus)\n",
    "    ax0 = [x[0][1] for x in nodes] # 绘制各个doc代表的点\n",
    "    ax1 = [x[1][1] for x in nodes]\n",
    "    # print(ax0)\n",
    "    # print(ax1)\n",
    "    plt.plot(ax0,ax1,'o')\n",
    "    plt.show()\n",
    "\n",
    "def get_stop_words_set(file_name):\n",
    "    with open(file_name,'r') as file:\n",
    "        return set([line.strip() for line in file])\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleanrn = re.compile('\\n')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    cleantext = re.sub(cleanrn,'',cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname, stopwordfile):\n",
    "        self.dirname = dirname\n",
    "        self.stop_list = get_stop_words_set(stopwordfile)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk(self.dirname):\n",
    "            for filename in files:\n",
    "                file_path = (root + '/' + filename)\n",
    "                for line in open(file_path,encoding='utf-8'):\n",
    "                    sline = line.strip()\n",
    "                    if sline == \"\":\n",
    "                        continue\n",
    "                    rline = cleanhtml(sline)\n",
    "                    tokenized_line = ' '.join(word_tokenize(rline))\n",
    "                    is_alpha_word_line = [word for word in\n",
    "                                          tokenized_line.lower().split()\n",
    "                                          if (word.isalpha() and word not in self.stop_list)]\n",
    "                    yield is_alpha_word_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-22 16:31:27,477 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data not found!\n",
      "found stopword file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-22 16:31:30,952 : INFO : adding document #10000 to Dictionary(15890 unique tokens: ['peru', 'bus', 'haq', 'surigao', 'conops']...)\n",
      "2017-09-22 16:31:34,351 : INFO : adding document #20000 to Dictionary(24116 unique tokens: ['peru', 'wmrdc', 'surigao', 'conops', 'idiotic']...)\n",
      "2017-09-22 16:31:37,898 : INFO : adding document #30000 to Dictionary(30721 unique tokens: ['peru', 'khankitigress', 'wmrdc', 'surigao', 'conops']...)\n",
      "2017-09-22 16:31:38,262 : INFO : built Dictionary(31259 unique tokens: ['peru', 'khankitigress', 'wmrdc', 'surigao', 'conops']...) from 31013 documents (total 216067 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "data_path = './test'\n",
    "if (os.path.isfile(data_path)):\n",
    "    print('file found')\n",
    "else :\n",
    "    print(\"data not found!\")\n",
    "    \n",
    "    \n",
    "stopword_path = './stopword.txt'\n",
    "if (os.path.isfile(stopword_path)):\n",
    "    print(\"found stopword file\")\n",
    "else :\n",
    "    print(\"stopword file not found!\")\n",
    "    \n",
    "\n",
    "sentences = MySentences(data_path, stopword_path)\n",
    "dictionary = gensim.corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-22 16:31:48,884 : INFO : collecting all words and their counts\n",
      "2017-09-22 16:31:48,887 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-22 16:31:52,075 : INFO : PROGRESS: at sentence #10000, processed 73839 words, keeping 15890 word types\n",
      "2017-09-22 16:31:55,202 : INFO : PROGRESS: at sentence #20000, processed 141854 words, keeping 24116 word types\n",
      "2017-09-22 16:32:00,888 : INFO : PROGRESS: at sentence #30000, processed 209180 words, keeping 30721 word types\n",
      "2017-09-22 16:32:01,666 : INFO : collected 31259 word types from a corpus of 216067 raw words and 31013 sentences\n",
      "2017-09-22 16:32:01,666 : INFO : Loading a fresh vocabulary\n",
      "2017-09-22 16:32:01,791 : INFO : min_count=1 retains 31259 unique words (100% of original 31259, drops 0)\n",
      "2017-09-22 16:32:01,792 : INFO : min_count=1 leaves 216067 word corpus (100% of original 216067, drops 0)\n",
      "2017-09-22 16:32:01,934 : INFO : deleting the raw counts dictionary of 31259 items\n",
      "2017-09-22 16:32:01,937 : INFO : sample=0.001 downsamples 21 most-common words\n",
      "2017-09-22 16:32:01,938 : INFO : downsampling leaves estimated 212225 word corpus (98.2% of prior 216067)\n",
      "2017-09-22 16:32:01,941 : INFO : estimated required memory for 31259 words and 100 dimensions: 40636700 bytes\n",
      "2017-09-22 16:32:02,095 : INFO : resetting layer weights\n",
      "2017-09-22 16:32:02,906 : INFO : training model with 4 workers on 31259 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-22 16:32:04,601 : INFO : PROGRESS: at 1.74% examples, 12942 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:06,035 : INFO : PROGRESS: at 4.36% examples, 16621 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:07,417 : INFO : PROGRESS: at 7.01% examples, 18118 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:08,425 : INFO : PROGRESS: at 8.89% examples, 18380 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:09,529 : INFO : PROGRESS: at 10.83% examples, 18279 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:10,587 : INFO : PROGRESS: at 12.72% examples, 18318 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:12,020 : INFO : PROGRESS: at 15.57% examples, 18674 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:13,062 : INFO : PROGRESS: at 17.51% examples, 18695 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:14,408 : INFO : PROGRESS: at 19.42% examples, 18209 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:15,683 : INFO : PROGRESS: at 22.08% examples, 18699 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:17,024 : INFO : PROGRESS: at 24.69% examples, 19011 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:18,701 : INFO : PROGRESS: at 27.35% examples, 18857 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:19,820 : INFO : PROGRESS: at 29.28% examples, 18770 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:20,885 : INFO : PROGRESS: at 31.20% examples, 18753 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:22,345 : INFO : PROGRESS: at 34.01% examples, 18862 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:23,774 : INFO : PROGRESS: at 36.91% examples, 18984 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:25,251 : INFO : PROGRESS: at 39.79% examples, 19051 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:26,584 : INFO : PROGRESS: at 42.42% examples, 19222 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:27,822 : INFO : PROGRESS: at 45.02% examples, 19450 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:29,237 : INFO : PROGRESS: at 47.70% examples, 19524 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:30,334 : INFO : PROGRESS: at 49.68% examples, 19458 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:31,363 : INFO : PROGRESS: at 51.57% examples, 19445 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:32,739 : INFO : PROGRESS: at 54.38% examples, 19537 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:34,246 : INFO : PROGRESS: at 57.28% examples, 19539 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:35,711 : INFO : PROGRESS: at 60.14% examples, 19565 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:36,851 : INFO : PROGRESS: at 61.88% examples, 19485 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:38,111 : INFO : PROGRESS: at 64.50% examples, 19622 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:39,475 : INFO : PROGRESS: at 67.15% examples, 19696 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:41,579 : INFO : PROGRESS: at 70.05% examples, 19386 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:43,034 : INFO : PROGRESS: at 72.87% examples, 19418 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:44,666 : INFO : PROGRESS: at 75.72% examples, 19365 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:46,224 : INFO : PROGRESS: at 78.62% examples, 19349 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:47,575 : INFO : PROGRESS: at 81.34% examples, 19423 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:48,861 : INFO : PROGRESS: at 83.96% examples, 19519 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:50,138 : INFO : PROGRESS: at 86.60% examples, 19615 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:51,613 : INFO : PROGRESS: at 89.44% examples, 19626 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:53,125 : INFO : PROGRESS: at 92.28% examples, 19622 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:54,153 : INFO : PROGRESS: at 94.17% examples, 19612 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:55,630 : INFO : PROGRESS: at 97.06% examples, 19621 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:56,727 : INFO : PROGRESS: at 98.99% examples, 19587 words/s, in_qsize 0, out_qsize 0\n",
      "2017-09-22 16:32:57,234 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-22 16:32:57,235 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-22 16:32:57,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-22 16:32:57,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-22 16:32:57,237 : INFO : training on 1080335 raw words (1061142 effective words) took 54.1s, 19597 effective words/s\n",
      "2017-09-22 16:32:57,238 : INFO : saving Word2Vec object under data/model/word2vec_gensim, separately None\n",
      "2017-09-22 16:32:57,239 : INFO : not storing attribute syn0norm\n",
      "2017-09-22 16:32:57,240 : INFO : not storing attribute cum_table\n",
      "2017-09-22 16:32:57,605 : INFO : saved data/model/word2vec_gensim\n",
      "2017-09-22 16:32:57,606 : INFO : storing vocabulary in data/model/vocabulary\n",
      "2017-09-22 16:32:57,791 : INFO : storing 31259x100 projection weights into data/model/word2vec_org\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Total procesing time: 70 seconds'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin = time()\n",
    "model = gensim.models.Word2Vec(sentences,\n",
    "                               size=100,\n",
    "                               window=5,\n",
    "                               min_count=1,\n",
    "                               workers=multiprocessing.cpu_count())\n",
    "model.save(\"data/model/word2vec_gensim\")\n",
    "model.wv.save_word2vec_format(\"data/model/word2vec_org\",\n",
    "                              \"data/model/vocabulary\",\n",
    "                              binary=False)\n",
    "\n",
    "end = time()\n",
    "print\n",
    "\"Total procesing time: %d seconds\" % (end - begin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-22 16:34:06,179 : INFO : collecting document frequencies\n",
      "2017-09-22 16:34:06,182 : INFO : PROGRESS: processing document #0\n",
      "2017-09-22 16:34:06,295 : INFO : PROGRESS: processing document #10000\n",
      "2017-09-22 16:34:06,355 : INFO : PROGRESS: processing document #20000\n",
      "2017-09-22 16:34:06,400 : INFO : PROGRESS: processing document #30000\n",
      "2017-09-22 16:34:06,407 : INFO : calculating IDF weights for 31013 documents and 31258 features (210360 matrix non-zeros)\n",
      "2017-09-22 16:34:06,469 : INFO : using symmetric alpha at 0.005\n",
      "2017-09-22 16:34:06,470 : INFO : using symmetric eta at 3.199078665344381e-05\n",
      "2017-09-22 16:34:06,486 : INFO : using serial LDA version on this node\n",
      "2017-09-22 16:34:33,310 : INFO : running online (single-pass) LDA training, 200 topics, 1 passes over the supplied corpus of 31013 documents, updating model once every 2000 documents, evaluating perplexity every 20000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-09-22 16:34:33,364 : INFO : PROGRESS: pass 0, at document #2000/31013\n",
      "2017-09-22 16:34:35,736 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:36,486 : INFO : topic #12 (0.005): 0.024*\"environmental\" + 0.023*\"unstoppable\" + 0.023*\"jerusalemcenter\" + 0.021*\"alive\" + 0.021*\"grandma\" + 0.020*\"plight\" + 0.019*\"geiger\" + 0.019*\"pants\" + 0.019*\"dangerous\" + 0.017*\"narcoditatorship\"\n",
      "2017-09-22 16:34:36,488 : INFO : topic #76 (0.005): 0.018*\"hearings\" + 0.017*\"news\" + 0.015*\"tug\" + 0.014*\"taseer\" + 0.014*\"homegrown\" + 0.013*\"spain\" + 0.013*\"massive\" + 0.013*\"wikileaks\" + 0.013*\"neal\" + 0.012*\"concerned\"\n",
      "2017-09-22 16:34:36,489 : INFO : topic #108 (0.005): 0.033*\"explosions\" + 0.031*\"report\" + 0.026*\"palestine\" + 0.019*\"tent\" + 0.019*\"quintan\" + 0.019*\"credit\" + 0.019*\"prior\" + 0.019*\"outlines\" + 0.018*\"photographer\" + 0.018*\"police\"\n",
      "2017-09-22 16:34:36,491 : INFO : topic #1 (0.005): 0.029*\"compo\" + 0.027*\"georgevorv\" + 0.022*\"nahyan\" + 0.019*\"extradition\" + 0.018*\"bout\" + 0.018*\"riskiest\" + 0.016*\"simultaneous\" + 0.016*\"security\" + 0.016*\"victims\" + 0.015*\"alimohamadi\"\n",
      "2017-09-22 16:34:36,492 : INFO : topic #18 (0.005): 0.018*\"yemen\" + 0.016*\"israels\" + 0.016*\"deeb\" + 0.016*\"held\" + 0.015*\"charge\" + 0.014*\"mardi\" + 0.014*\"gras\" + 0.013*\"screw\" + 0.013*\"tops\" + 0.013*\"closer\"\n",
      "2017-09-22 16:34:36,538 : INFO : topic diff=195.450729, rho=1.000000\n",
      "2017-09-22 16:34:36,610 : INFO : PROGRESS: pass 0, at document #4000/31013\n",
      "2017-09-22 16:34:38,952 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:39,763 : INFO : topic #15 (0.005): 0.051*\"men\" + 0.031*\"frm\" + 0.030*\"disaster\" + 0.023*\"comment\" + 0.022*\"end\" + 0.021*\"attempted\" + 0.016*\"reuters\" + 0.016*\"abortion\" + 0.015*\"iran\" + 0.011*\"charges\"\n",
      "2017-09-22 16:34:39,764 : INFO : topic #154 (0.005): 0.032*\"light\" + 0.031*\"debate\" + 0.030*\"flight\" + 0.025*\"paul\" + 0.024*\"leading\" + 0.018*\"aug\" + 0.017*\"world\" + 0.016*\"minds\" + 0.016*\"sound\" + 0.016*\"heavy\"\n",
      "2017-09-22 16:34:39,765 : INFO : topic #16 (0.005): 0.069*\"global\" + 0.031*\"pres\" + 0.027*\"power\" + 0.027*\"money\" + 0.025*\"afp\" + 0.022*\"massacre\" + 0.017*\"council\" + 0.015*\"murders\" + 0.013*\"britain\" + 0.011*\"posada\"\n",
      "2017-09-22 16:34:39,766 : INFO : topic #120 (0.005): 0.033*\"budget\" + 0.029*\"ganor\" + 0.025*\"beating\" + 0.024*\"fighting\" + 0.022*\"views\" + 0.019*\"expert\" + 0.019*\"starting\" + 0.018*\"defeat\" + 0.015*\"video\" + 0.014*\"psa\"\n",
      "2017-09-22 16:34:39,767 : INFO : topic #10 (0.005): 0.039*\"human\" + 0.033*\"rights\" + 0.023*\"rt\" + 0.022*\"fighting\" + 0.017*\"sahara\" + 0.017*\"intentions\" + 0.016*\"constitute\" + 0.016*\"fox\" + 0.014*\"turmoil\" + 0.013*\"war\"\n",
      "2017-09-22 16:34:39,809 : INFO : topic diff=0.634347, rho=0.707107\n",
      "2017-09-22 16:34:39,876 : INFO : PROGRESS: pass 0, at document #6000/31013\n",
      "2017-09-22 16:34:42,162 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:42,963 : INFO : topic #61 (0.005): 0.033*\"progress\" + 0.031*\"seek\" + 0.024*\"quran\" + 0.022*\"admit\" + 0.018*\"made\" + 0.018*\"islam\" + 0.017*\"bankruptcy\" + 0.014*\"terror\" + 0.014*\"roach\" + 0.012*\"hhshkmohd\"\n",
      "2017-09-22 16:34:42,964 : INFO : topic #44 (0.005): 0.063*\"end\" + 0.057*\"due\" + 0.044*\"latest\" + 0.033*\"friday\" + 0.029*\"terroris\" + 0.022*\"chasing\" + 0.020*\"dollars\" + 0.020*\"tax\" + 0.020*\"planes\" + 0.017*\"lose\"\n",
      "2017-09-22 16:34:42,965 : INFO : topic #132 (0.005): 0.127*\"america\" + 0.032*\"mexico\" + 0.029*\"statement\" + 0.027*\"mexican\" + 0.025*\"opposition\" + 0.019*\"latin\" + 0.019*\"rangers\" + 0.016*\"resistance\" + 0.015*\"observer\" + 0.014*\"mosques\"\n",
      "2017-09-22 16:34:42,966 : INFO : topic #170 (0.005): 0.050*\"internet\" + 0.044*\"makes\" + 0.037*\"animal\" + 0.030*\"legal\" + 0.028*\"plan\" + 0.019*\"activism\" + 0.017*\"side\" + 0.015*\"act\" + 0.014*\"taxes\" + 0.012*\"government\"\n",
      "2017-09-22 16:34:42,967 : INFO : topic #35 (0.005): 0.070*\"ties\" + 0.051*\"face\" + 0.034*\"death\" + 0.031*\"press\" + 0.027*\"forum\" + 0.025*\"charges\" + 0.022*\"refers\" + 0.020*\"thailand\" + 0.019*\"choice\" + 0.019*\"telegraph\"\n",
      "2017-09-22 16:34:43,004 : INFO : topic diff=0.515449, rho=0.577350\n",
      "2017-09-22 16:34:43,076 : INFO : PROGRESS: pass 0, at document #8000/31013\n",
      "2017-09-22 16:34:45,346 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:46,229 : INFO : topic #32 (0.005): 0.041*\"funding\" + 0.039*\"bad\" + 0.031*\"reported\" + 0.030*\"changed\" + 0.025*\"months\" + 0.024*\"humanitarian\" + 0.022*\"tired\" + 0.020*\"joke\" + 0.018*\"faith\" + 0.015*\"primary\"\n",
      "2017-09-22 16:34:46,230 : INFO : topic #42 (0.005): 0.058*\"afghan\" + 0.053*\"media\" + 0.050*\"special\" + 0.040*\"forces\" + 0.038*\"warfare\" + 0.036*\"o\" + 0.032*\"enterprise\" + 0.027*\"photography\" + 0.027*\"responses\" + 0.023*\"mission\"\n",
      "2017-09-22 16:34:46,231 : INFO : topic #160 (0.005): 0.057*\"meeting\" + 0.048*\"work\" + 0.044*\"secretary\" + 0.026*\"countering\" + 0.022*\"revealed\" + 0.021*\"practice\" + 0.020*\"cost\" + 0.019*\"price\" + 0.019*\"hillary\" + 0.019*\"shit\"\n",
      "2017-09-22 16:34:46,233 : INFO : topic #147 (0.005): 0.051*\"osama\" + 0.034*\"anti\" + 0.029*\"music\" + 0.028*\"senate\" + 0.026*\"test\" + 0.026*\"twin\" + 0.023*\"persecution\" + 0.019*\"passes\" + 0.019*\"bill\" + 0.017*\"brian\"\n",
      "2017-09-22 16:34:46,234 : INFO : topic #108 (0.005): 0.112*\"report\" + 0.039*\"give\" + 0.032*\"fbi\" + 0.032*\"protect\" + 0.031*\"approach\" + 0.031*\"palestine\" + 0.028*\"agents\" + 0.024*\"detained\" + 0.022*\"explosions\" + 0.020*\"credit\"\n",
      "2017-09-22 16:34:46,272 : INFO : topic diff=0.480685, rho=0.500000\n",
      "2017-09-22 16:34:46,346 : INFO : PROGRESS: pass 0, at document #10000/31013\n",
      "2017-09-22 16:34:48,607 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:49,561 : INFO : topic #13 (0.005): 0.051*\"night\" + 0.046*\"battle\" + 0.041*\"ago\" + 0.032*\"reacted\" + 0.024*\"apartheid\" + 0.021*\"smith\" + 0.021*\"authority\" + 0.019*\"prepares\" + 0.017*\"mohammed\" + 0.016*\"norwegians\"\n",
      "2017-09-22 16:34:49,562 : INFO : topic #169 (0.005): 0.070*\"history\" + 0.051*\"government\" + 0.045*\"sentenced\" + 0.038*\"somali\" + 0.034*\"convicted\" + 0.026*\"words\" + 0.025*\"swedish\" + 0.023*\"horror\" + 0.020*\"emotional\" + 0.019*\"anonymous\"\n",
      "2017-09-22 16:34:49,563 : INFO : topic #43 (0.005): 0.040*\"line\" + 0.033*\"advice\" + 0.031*\"view\" + 0.026*\"promise\" + 0.025*\"front\" + 0.024*\"attorney\" + 0.019*\"fsa\" + 0.019*\"describes\" + 0.018*\"mafia\" + 0.017*\"seized\"\n",
      "2017-09-22 16:34:49,565 : INFO : topic #62 (0.005): 0.043*\"saudi\" + 0.038*\"age\" + 0.033*\"arabia\" + 0.032*\"counterterrorism\" + 0.032*\"move\" + 0.030*\"main\" + 0.026*\"activities\" + 0.025*\"qualify\" + 0.024*\"defined\" + 0.023*\"irish\"\n",
      "2017-09-22 16:34:49,566 : INFO : topic #157 (0.005): 0.071*\"pakistani\" + 0.057*\"charged\" + 0.041*\"ppl\" + 0.038*\"caught\" + 0.038*\"officer\" + 0.036*\"recent\" + 0.029*\"site\" + 0.027*\"homegrown\" + 0.026*\"aiding\" + 0.022*\"creating\"\n",
      "2017-09-22 16:34:49,604 : INFO : topic diff=0.437241, rho=0.447214\n",
      "2017-09-22 16:34:49,678 : INFO : PROGRESS: pass 0, at document #12000/31013\n",
      "2017-09-22 16:34:52,008 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:52,790 : INFO : topic #121 (0.005): 0.050*\"strike\" + 0.048*\"decade\" + 0.040*\"team\" + 0.025*\"mr\" + 0.025*\"concern\" + 0.022*\"controversial\" + 0.020*\"empire\" + 0.017*\"kumar\" + 0.015*\"afghans\" + 0.014*\"professor\"\n",
      "2017-09-22 16:34:52,791 : INFO : topic #128 (0.005): 0.095*\"accused\" + 0.075*\"man\" + 0.058*\"system\" + 0.051*\"northern\" + 0.036*\"arms\" + 0.023*\"science\" + 0.023*\"happy\" + 0.023*\"launches\" + 0.018*\"ayers\" + 0.014*\"liberals\"\n",
      "2017-09-22 16:34:52,793 : INFO : topic #4 (0.005): 0.074*\"back\" + 0.067*\"thought\" + 0.033*\"admin\" + 0.024*\"posted\" + 0.022*\"killers\" + 0.021*\"afraid\" + 0.021*\"i\" + 0.021*\"covert\" + 0.020*\"sick\" + 0.019*\"dark\"\n",
      "2017-09-22 16:34:52,794 : INFO : topic #177 (0.005): 0.072*\"list\" + 0.032*\"photo\" + 0.024*\"learn\" + 0.024*\"travel\" + 0.020*\"removed\" + 0.019*\"award\" + 0.019*\"ways\" + 0.017*\"annual\" + 0.016*\"executed\" + 0.013*\"staged\"\n",
      "2017-09-22 16:34:52,795 : INFO : topic #20 (0.005): 0.072*\"twitter\" + 0.071*\"accuses\" + 0.033*\"google\" + 0.030*\"solution\" + 0.030*\"conspiracy\" + 0.025*\"facebook\" + 0.025*\"k\" + 0.023*\"addition\" + 0.020*\"j\" + 0.019*\"sought\"\n",
      "2017-09-22 16:34:52,831 : INFO : topic diff=0.433800, rho=0.408248\n",
      "2017-09-22 16:34:52,899 : INFO : PROGRESS: pass 0, at document #14000/31013\n",
      "2017-09-22 16:34:55,391 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:56,362 : INFO : topic #89 (0.005): 0.114*\"religion\" + 0.088*\"police\" + 0.038*\"conference\" + 0.032*\"happening\" + 0.032*\"hoax\" + 0.030*\"french\" + 0.025*\"lol\" + 0.023*\"game\" + 0.019*\"blind\" + 0.015*\"leaks\"\n",
      "2017-09-22 16:34:56,363 : INFO : topic #15 (0.005): 0.072*\"men\" + 0.059*\"bombs\" + 0.025*\"comment\" + 0.024*\"planning\" + 0.024*\"streets\" + 0.019*\"disaster\" + 0.019*\"frm\" + 0.018*\"abortion\" + 0.017*\"orlandosentinel\" + 0.014*\"engaged\"\n",
      "2017-09-22 16:34:56,364 : INFO : topic #41 (0.005): 0.144*\"obama\" + 0.107*\"boston\" + 0.053*\"house\" + 0.049*\"stand\" + 0.048*\"russia\" + 0.046*\"white\" + 0.042*\"research\" + 0.029*\"marathon\" + 0.020*\"barack\" + 0.017*\"nbc\"\n",
      "2017-09-22 16:34:56,365 : INFO : topic #108 (0.005): 0.155*\"report\" + 0.058*\"give\" + 0.057*\"protect\" + 0.036*\"palestine\" + 0.034*\"fbi\" + 0.030*\"approach\" + 0.023*\"detained\" + 0.022*\"agents\" + 0.020*\"israel\" + 0.020*\"town\"\n",
      "2017-09-22 16:34:56,367 : INFO : topic #93 (0.005): 0.087*\"definition\" + 0.059*\"reports\" + 0.044*\"week\" + 0.037*\"continues\" + 0.028*\"tsa\" + 0.027*\"safer\" + 0.024*\"determined\" + 0.024*\"eliminate\" + 0.023*\"fall\" + 0.019*\"august\"\n",
      "2017-09-22 16:34:56,404 : INFO : topic diff=0.436003, rho=0.377964\n",
      "2017-09-22 16:34:56,464 : INFO : PROGRESS: pass 0, at document #16000/31013\n",
      "2017-09-22 16:34:58,693 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:34:59,475 : INFO : topic #5 (0.005): 0.097*\"mqm\" + 0.065*\"karachi\" + 0.040*\"clear\" + 0.023*\"ppp\" + 0.022*\"local\" + 0.021*\"elections\" + 0.021*\"musharraf\" + 0.016*\"journal\" + 0.015*\"samaa\" + 0.014*\"usual\"\n",
      "2017-09-22 16:34:59,476 : INFO : topic #44 (0.005): 0.127*\"end\" + 0.070*\"due\" + 0.036*\"latest\" + 0.031*\"tax\" + 0.030*\"poll\" + 0.029*\"friday\" + 0.026*\"dollars\" + 0.022*\"wave\" + 0.021*\"teaching\" + 0.017*\"lose\"\n",
      "2017-09-22 16:34:59,477 : INFO : topic #85 (0.005): 0.143*\"act\" + 0.058*\"evidence\" + 0.033*\"patriot\" + 0.032*\"canadian\" + 0.031*\"watching\" + 0.025*\"limited\" + 0.021*\"lnyhbt\" + 0.018*\"phenomenon\" + 0.018*\"criticizing\" + 0.017*\"experience\"\n",
      "2017-09-22 16:34:59,478 : INFO : topic #165 (0.005): 0.058*\"bill\" + 0.049*\"staff\" + 0.037*\"hero\" + 0.029*\"paying\" + 0.027*\"resolve\" + 0.019*\"changing\" + 0.017*\"aqap\" + 0.015*\"diplomats\" + 0.014*\"delivers\" + 0.010*\"cd\"\n",
      "2017-09-22 16:34:59,479 : INFO : topic #30 (0.005): 0.078*\"truth\" + 0.051*\"voice\" + 0.045*\"david\" + 0.025*\"palestinians\" + 0.023*\"implications\" + 0.021*\"banner\" + 0.020*\"feel\" + 0.020*\"wins\" + 0.020*\"hindus\" + 0.015*\"editorial\"\n",
      "2017-09-22 16:34:59,515 : INFO : topic diff=0.398999, rho=0.353553\n",
      "2017-09-22 16:34:59,578 : INFO : PROGRESS: pass 0, at document #18000/31013\n",
      "2017-09-22 16:35:01,933 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:02,712 : INFO : topic #116 (0.005): 0.120*\"leaders\" + 0.046*\"worst\" + 0.033*\"low\" + 0.030*\"interest\" + 0.030*\"courage\" + 0.029*\"condemned\" + 0.027*\"algeria\" + 0.019*\"el\" + 0.019*\"reminder\" + 0.012*\"pushes\"\n",
      "2017-09-22 16:35:02,713 : INFO : topic #132 (0.005): 0.118*\"america\" + 0.088*\"nsa\" + 0.034*\"resistance\" + 0.033*\"statement\" + 0.028*\"opposition\" + 0.026*\"silence\" + 0.026*\"agent\" + 0.018*\"jazeera\" + 0.016*\"abuses\" + 0.014*\"teen\"\n",
      "2017-09-22 16:35:02,714 : INFO : topic #138 (0.005): 0.075*\"regime\" + 0.063*\"secret\" + 0.031*\"reporter\" + 0.028*\"justify\" + 0.027*\"service\" + 0.018*\"middleeast\" + 0.014*\"godfather\" + 0.014*\"buddhist\" + 0.014*\"borno\" + 0.012*\"violate\"\n",
      "2017-09-22 16:35:02,715 : INFO : topic #85 (0.005): 0.174*\"act\" + 0.061*\"evidence\" + 0.029*\"canadian\" + 0.027*\"tgdn\" + 0.026*\"watching\" + 0.024*\"experience\" + 0.024*\"lnyhbt\" + 0.022*\"patriot\" + 0.020*\"limited\" + 0.017*\"phenomenon\"\n",
      "2017-09-22 16:35:02,716 : INFO : topic #182 (0.005): 0.102*\"murder\" + 0.063*\"forms\" + 0.053*\"black\" + 0.047*\"jail\" + 0.045*\"mass\" + 0.035*\"question\" + 0.021*\"tough\" + 0.018*\"meets\" + 0.013*\"hour\" + 0.012*\"scam\"\n",
      "2017-09-22 16:35:02,752 : INFO : topic diff=0.411264, rho=0.333333\n",
      "2017-09-22 16:35:06,594 : INFO : -52.079 per-word bound, 4758635036226128.0 perplexity estimate based on a held-out corpus of 2000 documents with 4916 words\n",
      "2017-09-22 16:35:06,595 : INFO : PROGRESS: pass 0, at document #20000/31013\n",
      "2017-09-22 16:35:09,132 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:09,949 : INFO : topic #79 (0.005): 0.049*\"bring\" + 0.039*\"car\" + 0.033*\"brought\" + 0.031*\"christmas\" + 0.030*\"excuse\" + 0.029*\"struggle\" + 0.022*\"push\" + 0.020*\"central\" + 0.020*\"pat\" + 0.018*\"attacking\"\n",
      "2017-09-22 16:35:09,950 : INFO : topic #123 (0.005): 0.066*\"head\" + 0.058*\"arab\" + 0.047*\"gov\" + 0.033*\"thinks\" + 0.029*\"awareness\" + 0.021*\"material\" + 0.017*\"unsc\" + 0.015*\"punish\" + 0.015*\"shutdown\" + 0.014*\"breakingnews\"\n",
      "2017-09-22 16:35:09,951 : INFO : topic #86 (0.005): 0.088*\"post\" + 0.052*\"border\" + 0.040*\"assessment\" + 0.037*\"impact\" + 0.032*\"european\" + 0.021*\"perspective\" + 0.019*\"lawmakers\" + 0.019*\"beloved\" + 0.018*\"ethiopian\" + 0.017*\"operative\"\n",
      "2017-09-22 16:35:09,952 : INFO : topic #22 (0.005): 0.114*\"united\" + 0.073*\"arrest\" + 0.069*\"states\" + 0.043*\"deaths\" + 0.039*\"result\" + 0.036*\"hatred\" + 0.024*\"green\" + 0.016*\"republican\" + 0.012*\"require\" + 0.011*\"blamed\"\n",
      "2017-09-22 16:35:09,954 : INFO : topic #151 (0.005): 0.098*\"children\" + 0.048*\"responsible\" + 0.041*\"shot\" + 0.037*\"future\" + 0.027*\"pamela\" + 0.026*\"southern\" + 0.026*\"region\" + 0.025*\"geller\" + 0.024*\"blood\" + 0.023*\"ambassador\"\n",
      "2017-09-22 16:35:09,990 : INFO : topic diff=0.414160, rho=0.316228\n",
      "2017-09-22 16:35:10,043 : INFO : PROGRESS: pass 0, at document #22000/31013\n",
      "2017-09-22 16:35:12,258 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:13,033 : INFO : topic #31 (0.005): 0.079*\"evil\" + 0.060*\"blame\" + 0.047*\"related\" + 0.045*\"foxnews\" + 0.039*\"quot\" + 0.035*\"iraqi\" + 0.029*\"airlines\" + 0.025*\"accept\" + 0.013*\"hears\" + 0.013*\"fueling\"\n",
      "2017-09-22 16:35:13,033 : INFO : topic #105 (0.005): 0.072*\"trial\" + 0.058*\"issues\" + 0.046*\"review\" + 0.044*\"warning\" + 0.042*\"powers\" + 0.042*\"hope\" + 0.037*\"guardian\" + 0.030*\"student\" + 0.021*\"pentagon\" + 0.020*\"tires\"\n",
      "2017-09-22 16:35:13,035 : INFO : topic #143 (0.005): 0.081*\"guilty\" + 0.067*\"suspect\" + 0.052*\"things\" + 0.039*\"ap\" + 0.036*\"young\" + 0.027*\"racist\" + 0.022*\"water\" + 0.021*\"problems\" + 0.021*\"pleads\" + 0.019*\"mail\"\n",
      "2017-09-22 16:35:13,036 : INFO : topic #182 (0.005): 0.106*\"murder\" + 0.087*\"black\" + 0.067*\"forms\" + 0.059*\"mass\" + 0.043*\"jail\" + 0.036*\"question\" + 0.032*\"tough\" + 0.019*\"meets\" + 0.018*\"ivory\" + 0.012*\"insane\"\n",
      "2017-09-22 16:35:13,037 : INFO : topic #148 (0.005): 0.170*\"support\" + 0.040*\"stance\" + 0.038*\"steps\" + 0.028*\"condemnation\" + 0.026*\"massacre\" + 0.019*\"logic\" + 0.018*\"approves\" + 0.014*\"sauce\" + 0.011*\"wales\" + 0.011*\"ll\"\n",
      "2017-09-22 16:35:13,072 : INFO : topic diff=0.424709, rho=0.301511\n",
      "2017-09-22 16:35:13,121 : INFO : PROGRESS: pass 0, at document #24000/31013\n",
      "2017-09-22 16:35:15,269 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:16,043 : INFO : topic #121 (0.005): 0.040*\"strike\" + 0.040*\"team\" + 0.036*\"concern\" + 0.023*\"professor\" + 0.022*\"mr\" + 0.017*\"controversial\" + 0.016*\"scene\" + 0.015*\"questioned\" + 0.014*\"decade\" + 0.014*\"mall\"\n",
      "2017-09-22 16:35:16,044 : INFO : topic #194 (0.005): 0.145*\"found\" + 0.063*\"agree\" + 0.053*\"target\" + 0.042*\"set\" + 0.038*\"billion\" + 0.029*\"germany\" + 0.025*\"attacked\" + 0.023*\"gop\" + 0.020*\"hits\" + 0.019*\"hussain\"\n",
      "2017-09-22 16:35:16,046 : INFO : topic #154 (0.005): 0.083*\"hate\" + 0.062*\"debate\" + 0.033*\"considered\" + 0.029*\"leading\" + 0.025*\"i\" + 0.020*\"intellectual\" + 0.019*\"flight\" + 0.019*\"app\" + 0.017*\"numbers\" + 0.017*\"gang\"\n",
      "2017-09-22 16:35:16,047 : INFO : topic #113 (0.005): 0.143*\"counter\" + 0.077*\"arrests\" + 0.061*\"official\" + 0.050*\"information\" + 0.033*\"speaking\" + 0.030*\"critical\" + 0.018*\"map\" + 0.017*\"published\" + 0.016*\"vulnerable\" + 0.016*\"brazil\"\n",
      "2017-09-22 16:35:16,048 : INFO : topic #24 (0.005): 0.056*\"force\" + 0.054*\"federal\" + 0.048*\"racism\" + 0.037*\"war\" + 0.028*\"exist\" + 0.026*\"joint\" + 0.022*\"ignore\" + 0.022*\"claim\" + 0.022*\"families\" + 0.021*\"trust\"\n",
      "2017-09-22 16:35:16,085 : INFO : topic diff=0.387558, rho=0.288675\n",
      "2017-09-22 16:35:16,141 : INFO : PROGRESS: pass 0, at document #26000/31013\n",
      "2017-09-22 16:35:18,295 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:19,066 : INFO : topic #18 (0.005): 0.087*\"charge\" + 0.046*\"held\" + 0.040*\"ireland\" + 0.026*\"block\" + 0.016*\"cpl\" + 0.016*\"ahmed\" + 0.015*\"drawn\" + 0.014*\"arresting\" + 0.013*\"places\" + 0.011*\"images\"\n",
      "2017-09-22 16:35:19,067 : INFO : topic #126 (0.005): 0.056*\"airport\" + 0.047*\"agreed\" + 0.036*\"incident\" + 0.034*\"alleged\" + 0.028*\"account\" + 0.027*\"funny\" + 0.019*\"scott\" + 0.017*\"cairo\" + 0.016*\"date\" + 0.015*\"attend\"\n",
      "2017-09-22 16:35:19,069 : INFO : topic #91 (0.005): 0.052*\"biggest\" + 0.047*\"immigration\" + 0.047*\"wake\" + 0.046*\"business\" + 0.035*\"eradicate\" + 0.031*\"asia\" + 0.029*\"priority\" + 0.022*\"workers\" + 0.022*\"fake\" + 0.021*\"works\"\n",
      "2017-09-22 16:35:19,070 : INFO : topic #168 (0.005): 0.103*\"fear\" + 0.089*\"change\" + 0.056*\"climate\" + 0.051*\"violent\" + 0.041*\"conflict\" + 0.031*\"threatening\" + 0.030*\"hindu\" + 0.017*\"somalia\" + 0.016*\"coverage\" + 0.015*\"sanders\"\n",
      "2017-09-22 16:35:19,071 : INFO : topic #105 (0.005): 0.060*\"issues\" + 0.051*\"trial\" + 0.051*\"review\" + 0.046*\"powers\" + 0.041*\"guardian\" + 0.033*\"hope\" + 0.032*\"warning\" + 0.024*\"student\" + 0.023*\"hot\" + 0.020*\"pentagon\"\n",
      "2017-09-22 16:35:19,109 : INFO : topic diff=0.399980, rho=0.277350\n",
      "2017-09-22 16:35:19,161 : INFO : PROGRESS: pass 0, at document #28000/31013\n",
      "2017-09-22 16:35:21,317 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:22,095 : INFO : topic #146 (0.005): 0.075*\"guns\" + 0.065*\"financing\" + 0.057*\"bombing\" + 0.045*\"cia\" + 0.042*\"race\" + 0.037*\"hostage\" + 0.033*\"civil\" + 0.023*\"holding\" + 0.021*\"model\" + 0.020*\"process\"\n",
      "2017-09-22 16:35:22,096 : INFO : topic #113 (0.005): 0.135*\"counter\" + 0.078*\"official\" + 0.066*\"arrests\" + 0.042*\"information\" + 0.029*\"vulnerable\" + 0.027*\"speaking\" + 0.026*\"critical\" + 0.022*\"map\" + 0.019*\"heading\" + 0.014*\"assist\"\n",
      "2017-09-22 16:35:22,097 : INFO : topic #162 (0.005): 0.169*\"fight\" + 0.154*\"terrorists\" + 0.052*\"true\" + 0.040*\"bangladesh\" + 0.022*\"quetta\" + 0.020*\"interior\" + 0.017*\"backs\" + 0.014*\"yesterday\" + 0.012*\"shared\" + 0.012*\"coup\"\n",
      "2017-09-22 16:35:22,098 : INFO : topic #50 (0.005): 0.043*\"proud\" + 0.024*\"quick\" + 0.021*\"random\" + 0.020*\"presents\" + 0.017*\"fair\" + 0.017*\"burnt\" + 0.016*\"boss\" + 0.015*\"california\" + 0.015*\"drama\" + 0.014*\"loud\"\n",
      "2017-09-22 16:35:22,100 : INFO : topic #103 (0.005): 0.050*\"summit\" + 0.042*\"tech\" + 0.040*\"greatest\" + 0.039*\"combating\" + 0.039*\"deadly\" + 0.039*\"key\" + 0.036*\"investigated\" + 0.033*\"warming\" + 0.029*\"gave\" + 0.025*\"threat\"\n",
      "2017-09-22 16:35:22,136 : INFO : topic diff=0.370855, rho=0.267261\n",
      "2017-09-22 16:35:22,190 : INFO : PROGRESS: pass 0, at document #30000/31013\n",
      "2017-09-22 16:35:24,402 : INFO : merging changes from 2000 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:25,182 : INFO : topic #180 (0.005): 0.139*\"turkey\" + 0.080*\"person\" + 0.055*\"wrong\" + 0.040*\"asked\" + 0.030*\"islam\" + 0.029*\"completely\" + 0.024*\"forward\" + 0.018*\"civilization\" + 0.014*\"drop\" + 0.013*\"scholar\"\n",
      "2017-09-22 16:35:25,183 : INFO : topic #47 (0.005): 0.062*\"hit\" + 0.059*\"crash\" + 0.049*\"agenda\" + 0.048*\"community\" + 0.023*\"stabbing\" + 0.020*\"banning\" + 0.018*\"denial\" + 0.017*\"apple\" + 0.016*\"picture\" + 0.013*\"unfortunate\"\n",
      "2017-09-22 16:35:25,185 : INFO : topic #187 (0.005): 0.088*\"london\" + 0.041*\"concerns\" + 0.041*\"effective\" + 0.038*\"technology\" + 0.036*\"reporting\" + 0.025*\"event\" + 0.024*\"protesting\" + 0.017*\"exports\" + 0.017*\"spokesman\" + 0.016*\"universal\"\n",
      "2017-09-22 16:35:25,186 : INFO : topic #90 (0.005): 0.122*\"problem\" + 0.058*\"lives\" + 0.054*\"claims\" + 0.052*\"bomb\" + 0.040*\"kids\" + 0.030*\"rest\" + 0.026*\"enemies\" + 0.025*\"investigating\" + 0.014*\"damage\" + 0.014*\"produce\"\n",
      "2017-09-22 16:35:25,187 : INFO : topic #57 (0.005): 0.064*\"safe\" + 0.054*\"education\" + 0.050*\"saving\" + 0.040*\"create\" + 0.037*\"immigrants\" + 0.029*\"justification\" + 0.028*\"blames\" + 0.026*\"marriage\" + 0.021*\"build\" + 0.019*\"soros\"\n",
      "2017-09-22 16:35:25,224 : INFO : topic diff=0.385608, rho=0.258199\n",
      "2017-09-22 16:35:27,647 : INFO : -41.556 per-word bound, 3233546185304.7 perplexity estimate based on a held-out corpus of 1013 documents with 2460 words\n",
      "2017-09-22 16:35:27,648 : INFO : PROGRESS: pass 0, at document #31013/31013\n",
      "2017-09-22 16:35:28,847 : INFO : merging changes from 1013 documents into a model of 31013 documents\n",
      "2017-09-22 16:35:29,718 : INFO : topic #171 (0.005): 0.069*\"step\" + 0.057*\"major\" + 0.054*\"erdogan\" + 0.039*\"current\" + 0.036*\"meet\" + 0.032*\"normal\" + 0.030*\"affairs\" + 0.026*\"sounds\" + 0.022*\"knowledge\" + 0.021*\"jihadist\"\n",
      "2017-09-22 16:35:29,720 : INFO : topic #45 (0.005): 0.086*\"supports\" + 0.064*\"talks\" + 0.047*\"condemns\" + 0.040*\"putin\" + 0.031*\"easy\" + 0.029*\"cuba\" + 0.027*\"fights\" + 0.027*\"stopping\" + 0.024*\"worried\" + 0.019*\"admitted\"\n",
      "2017-09-22 16:35:29,721 : INFO : topic #27 (0.005): 0.101*\"national\" + 0.091*\"victim\" + 0.044*\"jews\" + 0.031*\"exists\" + 0.030*\"dc\" + 0.026*\"ministry\" + 0.024*\"nazi\" + 0.024*\"hurt\" + 0.014*\"security\" + 0.014*\"eliminating\"\n",
      "2017-09-22 16:35:29,722 : INFO : topic #160 (0.005): 0.090*\"work\" + 0.060*\"hillary\" + 0.049*\"meeting\" + 0.031*\"shit\" + 0.023*\"countering\" + 0.021*\"secretary\" + 0.019*\"price\" + 0.018*\"elephant\" + 0.016*\"muslim\" + 0.016*\"pakarmy\"\n",
      "2017-09-22 16:35:29,723 : INFO : topic #12 (0.005): 0.091*\"woman\" + 0.068*\"dangerous\" + 0.038*\"promises\" + 0.038*\"health\" + 0.034*\"saudis\" + 0.032*\"guard\" + 0.024*\"protecting\" + 0.021*\"tories\" + 0.017*\"medical\" + 0.014*\"presidency\"\n",
      "2017-09-22 16:35:29,760 : INFO : topic diff=0.292472, rho=0.250000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "tfidf_m = tfidf_model[corpus]\n",
    "lda = gensim.models.LdaModel(tfidf_m, id2word=dictionary, num_topics=200)\n",
    "corpus_lda = lda[tfidf_m]\n",
    "lda_csc_matrix = gensim.matutils.corpus2csc(corpus_lda).transpose()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters=10)\n",
    "kmean.fit(lda_csc_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-710695a011a9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-710695a011a9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    doc2 = MySentences('./testContent.txt'， stopword_path)\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "doc2 = MySentences('./testContent.txt'， stopword_path)\n",
    "copDoc2 = dictionary.doc2bow(doc2)\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "lda = gensim.models.LdaModel(corpus_tfidf, num_topics=50, id2word=dictionary)\n",
    "corpus_lda = lda[corpus_tfidf]\n",
    "Show2dCorpora(corpus_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-12-6d24b69fd2f0>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6d24b69fd2f0>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    weight = lda_csc_matrix.toArray()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " from sklearn.decomposition import PCA\n",
    "\n",
    "    weight = lda_csc_matrix.toArray()\n",
    "    pca = PCA(n_components=2)  # 输出两维\n",
    "    newData = pca.fit_transform(weight)  # 载入N维\n",
    "    print(newData)\n",
    "\n",
    "    # 5A景区\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    i = 0\n",
    "    while i < 400:\n",
    "        x1.append(newData[i][0])\n",
    "        y1.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 动物\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    i = 400\n",
    "    while i < 600:\n",
    "        x2.append(newData[i][0])\n",
    "        y2.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 人物\n",
    "    x3 = []\n",
    "    y3 = []\n",
    "    i = 600\n",
    "    while i < 800:\n",
    "        x3.append(newData[i][0])\n",
    "        y3.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 国家\n",
    "    x4 = []\n",
    "    y4 = []\n",
    "    i = 800\n",
    "    while i < 1000:\n",
    "        x4.append(newData[i][0])\n",
    "        y4.append(newData[i][1])\n",
    "        i += 1\n",
    "\n",
    "        # 四种颜色 红 绿 蓝 黑\n",
    "    PCA.plt.plot(x1, y1, 'or')\n",
    "    PCA.plt.plot(x2, y2, 'og')\n",
    "    PCA.plt.plot(x3, y3, 'ob')\n",
    "    PCA.plt.plot(x4, y4, 'ok')\n",
    "    PCA.plt.show()\n",
    "\n",
    "    data_path = sys.argv[1]\n",
    "    begin = time()\n",
    "\n",
    "    sentences = MySentences(data_path)\n",
    "    dictionary = gensim.corpora.Dictionary(sentences)\n",
    "    corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "    model = gensim.models.Word2Vec(sentences,\n",
    "                                   size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=5,\n",
    "                                   workers=multiprocessing.cpu_count())\n",
    "\n",
    "    model.save(\"data/model/word2vec_gensim\")\n",
    "    model.wv.save_word2vec_format(\"data/model/word2vec_org\",\n",
    "                                  \"data/model/vocabulary\",\n",
    "                                  binary=False)\n",
    "\n",
    "    end = time()\n",
    "    print\n",
    "    \"Total procesing time: %d seconds\" % (end - begin)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
